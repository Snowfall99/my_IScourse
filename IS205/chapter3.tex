\documentclass[a4paper, 12pt]{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}

\title{Chapter 3: 信道与信道容量}
\author{Xuan}
\begin{document}
    \maketitle
    \section{信道的基本概念}
    \subsection{二进制离散信道(BSC)}
    该信道模型的输入和输出信号的符号数都是2，即$X\in A={0，1}$和$Y\in B={0, 1}$，转移概率为
    \begin{equation}
        \begin{aligned}
            p(Y=0|X=1)&=p(Y=1|X=0)=p\\
            p(Y=1|X=1)&=p(Y=0|X=0)=1-p
        \end{aligned}
    \end{equation}
    \subsection{加性高斯白噪声信道(AWGN)}
    \[Y=X+G\]
    G是一个零均值、方差为$\sigma^2$的高斯随机变量，当$X=a_i$给定后，Y是一个均值为$a_i$、方差为$\sigma^2$的高斯随机变量
    \[p_Y(y|a_i)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-a_i)^2/2\sigma^2}\]
    \section{信道}
    信道可以看成是转移概率

    对于信息$M\in\mathcal{M}$，传输速率$R=\frac{log(\mathcal{M})}{n}$，信道传输的过程即可表示为：
    \[M\rightarrow x^n(M)\xrightarrow{p(y|x)}y^n \rightarrow \hat{M}\]
    
    \begin{enumerate}
        \item 设计一个方案，该方案可以达到某传输概率
        \item 证明超出该传输速率无法传输 $\iff$ 能够传输的都不超过这个速率
    \end{enumerate}
    \subsection{1.设计方案}
    \subsubsection{典型序列}
    Define: $x^n$ is (n, $\varepsilon$) typical, when $|N(x|x^n)-p(x)|<\varepsilon n$, for all
    $x\in \mathcal{X}$, where $N(x|x^n)$ is the empirical distribution.

    $Pr(x^n is typical)\rightarrow 1$, when n $\rightarrow \infty$, which can be proved by Law of Large Numbers
    \subsection{典型集}
    Set $T(n, \varepsilon)$ 为典型序列的集合

    \begin{enumerate}
        \item $Pr(x^n \in T(n, \varepsilon))\rightarrow 1$
        \item $|T(n,\varepsilon)|\approx2^{nH(x)}$
        \subitem 2.1 $x^n\in T(\varepsilon, n), p(X^n=x^n)\approx 2^{-nH(x)}$
    \end{enumerate}
    \subsubsection{Proof}
    \[
        p(x^n)=\prod_{i=1}^{n}p(x_i)=\prod_{x\in \mathcal{X}}p(x)^{N(x|x^n)n} (*)
    \]
    $\Rightarrow$所有典型序列的概率都差不多大
    \[
        log(*)=\sum_{x\in \mathcal{X}}nN(x|x^n)logp(x)\approx n\sum_{x\in \mathcal{X}}p(x)logp(x)=-nH(x)
    \]
    Pr($x^n$: $x^n$ is typical) = * $\approx 2^{-nH(x)}$

    \subsection{典型集和散度的关系}
    $Pr(x^n\in T(n, \varepsilon))=?$
    
    $x^n\in T(n, \varepsilon), N(x|x^n)~p(x)$

    \begin{equation}
        \begin{aligned}
            Pr(x^n)&=\prod q(x)^{nN(x|x^n)}\\
            &\approx \prod q(x)^{np(x)}\\
            &=2^{log\prod q(x)^{np(x)}}\\
            &=2^{np(x)\sum logq(x)}\\
            &=2^{-np(x)log\frac{1}{q(x)}}
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            Pr(T(n, \varepsilon)) &= \sum_{x^n \in T(\varepsilon, n)}Pr(x^n)\\
            &\approx 2^{-np(x)log p(x)}*2^{-np(x)log \frac{1}{q(x)}}\\
            &=2^{-nD(p||q)}
        \end{aligned}
    \end{equation}
    \subsection{条件典型集}
    典型条件集的大小：$2^{nH(Y|X)}$    
    \section{随机码簿(Random Codebook)}
    $X-Y,p(y|x)$，n长编码，$|M|\approx 2^{nI(X;Y)}$

    具体步骤如下：
    \begin{enumerate}
        \item 生成码簿：给定一个概率$p(x)$，按$p(x)，i.i.d.$生成$x^n(i)$序列，重复$2^{nI(X;Y)}$次
        \item 发送$x^n(M)$
        \item 解码：$y^n$查表寻找$(x^n(i),y^n)$联合典型
        \item 错误率分析：
        \begin{enumerate}
            \item 错误一：发送$x^n$，但收到的$y^n$和$x^n$不联合典型
            \item 错误二：发送$x^n$，收到$y^n$，但存在$(x^n)'$与$y^n$联合典型
        \end{enumerate} 
    \end{enumerate}
    \[p(mistake1 \cup mistake2) \le p(mistake1) + p(mistake2)\]
    \begin{enumerate}
        \item $p(mistake1)\rightarrow 0$, when $n \rightarrow \infty$(由大数定律可知)
        \item $p(mistake2)\rightarrow 0$, when $|M|\le 2^{nI(X;Y)}$
    \end{enumerate}
    \begin{equation}
        \begin{aligned}
            p(mistake2) &\le \sum_{x^n\neq (x^n)'}p((x^n)', y^n)\\
            &=2^{n[I(X;Y)-\varepsilon]}-2^{-nI(X;Y)}
        \end{aligned}
    \end{equation}
    \section{Converse theory of channel capacity}
    假设$|M|$等概率发生
    \begin{equation}
        \begin{aligned}
            R&=\frac{log|M|}{n}\\
            nR&=log|M|=H(M)\\
            &=H(M|Y^n)+I(M;Y^n)
        \end{aligned}
    \end{equation}
    其中
    \begin{equation}
        \begin{aligned}
            I(M;Y^n)&=I(X^n;Y^n)\\
            &=H(Y^n)-H(Y^n|X^n)\\
            &\le \sum_{i=0}^{n}H(y_i)-H(Y^n|X^n)\\
            &=\sum H(y_i) - \sum H(y_i|X^n, y_1,...,y_n)(Chain Rule)\\
            &=\sum H(y_i)- \sum H(y_i|x_i)\\
            &=\sum I(x_i;y_i)\\
            &\le nmax_{p(x_i)}I(X;Y)
        \end{aligned}
    \end{equation}
    \subsection{Fano's Inequality}
    For Markov Chain, $X\rightarrow Y \rightarrow \hat{X}$
    \[
        H(X|\hat{X})\le H(Pe) + log(|X|-1) \le 1 + log(|H|-1)    
    \]
    where $Pe$ is the probability of making errors, $H(Pe)$ obey 0-1 distribution, 
    such that $H(Pe) \le 1$
    \subsubsection{Proof of Fano's Inequality}
    Import Indicator Variable E, where has the property:
    \[
        E=\left\{
            \begin{array}{rcl}
                0,X=\hat{X}\\
                1,X\neq\hat{X}
            \end{array}
            \right.
    \]
    such that $H(E|X,\hat{X}) = 0$
    \begin{equation}
        \begin{aligned}
            H(X|\hat{X})&=H(X|\hat{X}) + H(E|X,\hat{X})\\
            &=H(E,X|\hat{X})\\
            &=H(X|E, \hat{X})+H(E|\hat{X})\\
            &=(Pe)H(X|E=1,\hat{X}) + (1-Pe)H(X|E=0,\hat{X}) + H(E|\hat{X})\\
            &\le (Pe)log(|X|-1) + H(Pe)
        \end{aligned}
    \end{equation}
    \[
        H(X|E=0,\hat{X}) = 0    
    \]
    \subsection{$R\le maxI(X;Y)$}
    From equation (5), together with equation (6) and (7), we can get
    \begin{equation}
        \begin{aligned}
            H(M|Y^n) + I(M;Y^n) &\le (Pe)log(|M|) + nmaxI(X;Y)\\
            (1-Pe)R &\le maxI(X;Y)\\
        \end{aligned}
    \end{equation}
    Because $Pe \rightarrow 0$, so that we can conclude that $R\le max_{p(x_i)}I(X;Y)$

\end{document}