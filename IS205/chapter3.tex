\documentclass[a4paper, 12pt]{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}

\title{Chapter 3: 信道与信道容量}
\author{Xuan}
\begin{document}
    \maketitle
    \section{信道的基本概念}
    \subsection{二进制离散信道(BSC)}
    该信道模型的输入和输出信号的符号数都是2，即$X\in A={0，1}$和$Y\in B={0, 1}$，转移概率为
    \begin{equation}
        \begin{aligned}
            p(Y=0|X=1)&=p(Y=1|X=0)=p\\
            p(Y=1|X=1)&=p(Y=0|X=0)=1-p
        \end{aligned}
    \end{equation}
    \subsection{加性高斯白噪声信道(AWGN)}
    \[Y=X+G\]
    G是一个零均值、方差为$\sigma^2$的高斯随机变量，当$X=a_i$给定后，Y是一个均值为$a_i$、方差为$\sigma^2$的高斯随机变量
    \[p_Y(y|a_i)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(y-a_i)^2/2\sigma^2}\]
    \section{信道}
    信道可以看成是转移概率

    对于信息$M\in\mathcal{M}$，传输速率$R=\frac{log(\mathcal{M})}{n}$，信道传输的过程即可表示为：
    \[M\rightarrow x^n(M)\xrightarrow{p(y|x)}y^n \rightarrow \hat{M}\]
    
    \begin{enumerate}
        \item 设计一个方案，该方案可以达到某传输概率
        \item 证明超出该传输速率无法传输 $\iff$ 能够传输的都不超过这个速率
    \end{enumerate}
    \subsection{1.设计方案}
    \subsubsection{典型序列}
    Define: $x^n$ is (n, $\varepsilon$) typical, when $|N(x|x^n)-p(x)|<\varepsilon n$, for all
    $x\in \mathcal{X}$, where $N(x|x^n)$ is the empirical distribution.

    $Pr(x^n is typical)\rightarrow 1$, when n $\rightarrow \infty$, which can be proved by Law of Large Numbers
    \subsection{典型集}
    Set $T(n, \varepsilon)$ 为典型序列的集合

    \begin{enumerate}
        \item $Pr(x^n \in T(n, \varepsilon))\rightarrow 1$
        \item $|T(n,\varepsilon)|\approx2^{nH(x)}$
        \subitem 2.1 $x^n\in T(\varepsilon, n), p(X^n=x^n)\approx 2^{-nH(x)}$
    \end{enumerate}
    \subsubsection{Proof}
    \[
        p(x^n)=\prod_{i=1}^{n}p(x_i)=\prod_{x\in \mathcal{X}}p(x)^{N(x|x^n)n} (*)
    \]
    $\Rightarrow$所有典型序列的概率都差不多大
    \[
        log(*)=\sum_{x\in \mathcal{X}}nN(x|x^n)logp(x)\approx n\sum_{x\in \mathcal{X}}p(x)logp(x)=-nH(x)
    \]
    Pr($x^n$: $x^n$ is typical) = * $\approx 2^{-nH(x)}$

    \subsection{典型集和散度的关系}
    $Pr(x^n\in T(n, \varepsilon))=?$
    
    $x^n\in T(n, \varepsilon), N(x|x^n)~p(x)$

    \begin{equation}
        \begin{aligned}
            Pr(x^n)&=\prod q(x)^{nN(x|x^n)}\\
            &\approx \prod q(x)^{np(x)}\\
            &=2^{log\prod q(x)^{np(x)}}\\
            &=2^{np(x)\sum logq(x)}\\
            &=2^{-np(x)log\frac{1}{q(x)}}
        \end{aligned}
    \end{equation}
    \begin{equation}
        \begin{aligned}
            Pr(T(n, \varepsilon)) &= \sum_{x^n \in T(\varepsilon, n)}Pr(x^n)\\
            &\approx 2^{-np(x)log p(x)}*2^{-np(x)log \frac{1}{q(x)}}\\
            &=2^{-nD(p||q)}
        \end{aligned}
    \end{equation}
    \subsection{条件典型集}
    典型条件集的大小：$2^{nH(Y|X)}$    
    \section{随机码簿(Random Codebook)}
    $X-Y,p(y|x)$，n长编码，$|M|\approx 2^{nI(X;Y)}$

    具体步骤如下：
    \begin{enumerate}
        \item 生成码簿：给定一个概率$p(x)$，按$p(x)，i.i.d.$生成$x^n(i)$序列，重复$2^{nI(X;Y)}$次
        \item 发送$x^n(M)$
        \item 解码：$y^n$查表寻找$(x^n(i),y^n)$联合典型
        \item 错误率分析：
        \begin{enumerate}
            \item 错误一：发送$x^n$，但收到的$y^n$和$x^n$不联合典型
            \item 错误二：发送$x^n$，收到$y^n$，但存在$(x^n)'$与$y^n$联合典型
        \end{enumerate} 
    \end{enumerate}
    \[p(mistake1 \cup mistake2) \le p(mistake1) + p(mistake2)\]
    \begin{enumerate}
        \item $p(mistake1)\rightarrow 0$, when $n \rightarrow \infty$(由大数定律可知)
        \item $p(mistake2)\rightarrow 0$, when $|M|\le 2^{nI(X;Y)}$
    \end{enumerate}
    \begin{equation}
        \begin{aligned}
            p(mistake2) &\le \sum_{x^n\neq (x^n)'}p((x^n)', y^n)\\
            &=2^{n[I(X;Y)-\varepsilon]}-2^{-nI(X;Y)}
        \end{aligned}
    \end{equation}
    \section{Converse theory of channel capacity}
    假设$|M|$等概率发生
    \begin{equation}
        \begin{aligned}
            R&=\frac{log|M|}{n}\\
            nR&=log|M|=H(M)\\
            &=H(M|Y^n)+I(M;Y^n)
        \end{aligned}
    \end{equation}
    其中
    \begin{equation}
        \begin{aligned}
            I(M;Y^n)&=I(X^n;Y^n)\\
            &=H(Y^n)-H(Y^n|X^n)\\
            &\le \sum_{i=0}^{n}H(y_i)-H(Y^n|X^n)\\
            &=\sum H(y_i) - \sum H(y_i|X^n, y_1,...,y_n)(Chain Rule)\\
            &=\sum H(y_i)- \sum H(y_i|x_i)\\
            &=\sum I(x_i;y_i)\\
            &\le nmax_{p(x_i)}I(X;Y)
        \end{aligned}
    \end{equation}
    \subsection{Fano's Inequality}
    For Markov Chain, $X\rightarrow Y \rightarrow \hat{X}$
    \[
        H(X|\hat{X})\le H(Pe) + log(|X|-1) \le 1 + log(|H|-1)    
    \]
    where $Pe$ is the probability of making errors, $H(Pe)$ obey 0-1 distribution, 
    such that $H(Pe) \le 1$
    \subsubsection{Proof of Fano's Inequality}
    Import Indicator Variable E, where has the property:
    \[
        E=\left\{
            \begin{array}{rcl}
                0,X=\hat{X}\\
                1,X\neq\hat{X}
            \end{array}
            \right.
    \]
    such that $H(E|X,\hat{X}) = 0$
    \begin{equation}
        \begin{aligned}
            H(X|\hat{X})&=H(X|\hat{X}) + H(E|X,\hat{X})\\
            &=H(E,X|\hat{X})\\
            &=H(X|E, \hat{X})+H(E|\hat{X})\\
            &=(Pe)H(X|E=1,\hat{X}) + (1-Pe)H(X|E=0,\hat{X}) + H(E|\hat{X})\\
            &\le (Pe)log(|X|-1) + H(Pe)
        \end{aligned}
    \end{equation}
    \[
        H(X|E=0,\hat{X}) = 0    
    \]
    \subsection{$R\le maxI(X;Y)$}
    From equation (5), together with equation (6) and (7), we can get
    \begin{equation}
        \begin{aligned}
            H(M|Y^n) + I(M;Y^n) &\le (Pe)log(|M|) + nmaxI(X;Y)\\
            (1-Pe)R &\le maxI(X;Y)\\
        \end{aligned}
    \end{equation}
    Because $Pe \rightarrow 0$, so that we can conclude that $R\le max_{p(x_i)}I(X;Y)$
    \section{Extension: 一阶马尔可夫链的概率}
    $X_1-X_2-...-X_n$, 
    \[
        P_{x_n}|P_{x_1}=P_{x_n}|P_{x_{n-1}}...P_{x_2}|P_{x_1}=(P_{x_{i+1}}|P_{x_{i}})^{n-1}  
    \]
    稳态：转移矩阵的特征向量
    \section{微分熵的性质}
    \begin{enumerate}
        \item $h(x)=h(x+C)$，C是常数
        \item $h(aX)=h(X)+log|a|$
        \begin{enumerate}
            \item $h(A\vec{x})=h(\vec{x})+log|A|$
        \end{enumerate}
    \end{enumerate}
    \subsection{性质2的证明}
    令$Y=aX$，则有$dy=adX$，故$P(Y\le y)=P(X\le \frac{y}{a})$
    \[
        P_Y(y)=\frac{dP(Y\le y)}{dy}=\frac{dP(X\le \frac{y}{a})}{d\frac{y}{a}}*\frac{1}{a}=f_x(\frac{y}{a})*\frac{1}{a}    
    \]
    \begin{equation}
        \begin{aligned}
            -\int p(y)logp(y)dy&=-\int \frac{1}{a}f_x(\frac{y}{a})log\frac{1}{a}f_x(\frac{y}{a})dy\\
            &=-\int f_x(\frac{y}{a})log\frac{1}{a}f_x(\frac{y}{a})d\frac{y}{a}\\
            &=-\int f_x(x)log\frac{1}{a}f_x(x)dx\\
            &=-\int f_x(x)log f_x(x)dx-\int f_x(x)log\frac{1}{a}dx\\
            &=-\int f_x(x)log f_x(x)dx-log|a|
        \end{aligned}
    \end{equation}
    \section{连续信道}
    \paragraph{Example}
    对于$X \sim N(0, \sigma^2)$，有$h(x) = \frac{1}{2}log(2 \pi e \sigma^2)$.  
    \newline 则对于$aX \sim N(0, a^2 \sigma^2)$，有$h(ax) = \frac{1}{2}log(2 \pi e a^2 \sigma^2) = \frac{1}{2}log(2\pi e \sigma^2)+\frac{1}{2}log a^2 = \frac{1}{2}log(2\pi e \sigma^2) + loga$
    \subsection{加性噪声信道(Additative Noise Channel)}
    x与z相互独立
    \newline $p(y) = p(x+z)$
    \newline $P(Y \le y) = P(X+Z \le y) = \int_{X+Z \le y}p(x,z)dxdz = \int_{X+Z \le y}p(x)p(z)dxdz$
    \begin{enumerate}
        \item 对任何X,Z是高斯分布时，最小化信道容量$I(X;Y)$，因为该情况下混乱程度最大（Proof: Entropy Power Inequality）
        \item 在方差一定的情况下，如果Z是高斯分布，则X也是高斯分布时，最大化信道容量
    \end{enumerate}
    \subsubsection{证明第二条}
    \begin{equation}
        \begin{aligned}
            I(Y;X) &= I(X+Z; X) = H(x+z) - H(x+z|x)\\
            &=H(x+z)-H(z)
        \end{aligned}
    \end{equation}
    $D(x), Var(x)$为给定值的情况下，$Var(x+z)=Var(x)+Var(z)$为定值
    \newline 因为当X+Z为高斯分布时，H(X+Z)取到最大值，由于Z为高斯分布，故X也服从高斯分布

    \subsection{信道容量不等式}
    $X \sim N(0, P), Z \sim N(0, N)$
    \begin{equation}
        \begin{aligned}
            C &= I(X; Y) = h(x+z) - h(z)\\
            &= \frac{1}{2}log(2 \pi e (P+N)) - \frac{1}{2}log(2 \pi eN)\\
            &= \frac{1}{2}(\frac{2\pi e(P+N)}{2\pi eN})\\
            &= \frac{1}{2}log(\frac{P+N}{N})\\
            &= \frac{1}{2}log(1+\frac{P}{N})
        \end{aligned}
    \end{equation}
    信噪比SNR：$\frac{P}{N}$
    \subsection{注水法(Water-Filling)}
    $x_1,...,x_N$相互独立，$Var(z_i)=N_i$，$\sum Var(X_i)=P_s$，问$max\sum_{i=1}^nI(X_i;Y_i)$
    \newline $\frac{1}{2}log(1+\frac{P_i}{N_i})\Leftrightarrow max\sum_{i=1}^n\frac{1}{2}log(1+\frac{P_i}{N_i}), s.t. P_1+...+P_N=P_s$
    \newline $L(\vec{P})=\sum_{i=1}^n\frac{1}{2}log(1+\frac{P_i}{N_i})-\lambda(\sum_{i=1}^nP_i-P_s)$
    \begin{equation}
        \begin{aligned}
            \frac{\partial L}{\partial P_i}&=\frac{\partial \sum_{i=1}^n\frac{1}{2}log(1+\frac{P_i}{N_i})-\lambda(\sum_{i=1}^nP_i-P_s)}{\partial P_i}\\
            &=\frac{1}{2}\frac{1}{P_i+N_i}-\lambda
            &=0
        \end{aligned}
    \end{equation}
    As a result, $P_i+N_i=\frac{1}{2\lambda}$, so that $\sum P_I+N_i = \frac{n}{2 \lambda}$
\end{document}